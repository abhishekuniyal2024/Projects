{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275ba469-dae0-45a3-84db-4474a9e70c27",
   "metadata": {},
   "source": [
    "# Face Mask detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861a2a6c-1aec-4218-a4ef-7091b674bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Import Libraries and Define Paths\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "dataset_path = r\"E:\\Machine Learning\\Face Mask\"\n",
    "model_save_path = r\"E:\\Machine Learning\\face_mask_model\"\n",
    "\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8a4f21-9247-44fa-a797-341acb1e4928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5960 images belonging to 2 classes.\n",
      "Found 1490 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Data Preprocessing and Augmentation\n",
    "IMG_SIZE = 64  \n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.20\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.20)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f316a1-846f-4daa-a7fd-dbc529cd274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Build and Compile the CNN Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29615cde-2bd4-4f62-94b7-4558d601b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model...\n",
      "Epoch 1/10\n",
      "270/745 [=========>....................] - ETA: 31s - loss: 0.5837 - accuracy: 0.6981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\ducat\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745/745 [==============================] - 74s 80ms/step - loss: 0.4865 - accuracy: 0.7725 - val_loss: 0.3048 - val_accuracy: 0.8763\n",
      "Epoch 2/10\n",
      "745/745 [==============================] - 15s 20ms/step - loss: 0.3885 - accuracy: 0.8349 - val_loss: 0.2319 - val_accuracy: 0.9194\n",
      "Epoch 3/10\n",
      "745/745 [==============================] - 15s 20ms/step - loss: 0.3394 - accuracy: 0.8602 - val_loss: 0.1814 - val_accuracy: 0.9368\n",
      "Epoch 4/10\n",
      "745/745 [==============================] - 15s 20ms/step - loss: 0.2871 - accuracy: 0.8826 - val_loss: 0.1714 - val_accuracy: 0.9368\n",
      "Epoch 5/10\n",
      "745/745 [==============================] - 15s 20ms/step - loss: 0.2471 - accuracy: 0.9042 - val_loss: 0.1538 - val_accuracy: 0.9476\n",
      "Epoch 6/10\n",
      "745/745 [==============================] - 15s 20ms/step - loss: 0.2307 - accuracy: 0.9146 - val_loss: 0.1394 - val_accuracy: 0.9476\n",
      "Epoch 7/10\n",
      "745/745 [==============================] - 15s 21ms/step - loss: 0.2150 - accuracy: 0.9208 - val_loss: 0.1404 - val_accuracy: 0.9503\n",
      "Epoch 8/10\n",
      "745/745 [==============================] - 15s 21ms/step - loss: 0.2010 - accuracy: 0.9257 - val_loss: 0.1332 - val_accuracy: 0.9523\n",
      "Epoch 9/10\n",
      "745/745 [==============================] - 15s 21ms/step - loss: 0.1904 - accuracy: 0.9320 - val_loss: 0.1490 - val_accuracy: 0.9476\n",
      "Epoch 10/10\n",
      "745/745 [==============================] - 16s 22ms/step - loss: 0.1864 - accuracy: 0.9305 - val_loss: 0.1499 - val_accuracy: 0.9449\n",
      "Class indices saved to E:\\Machine Learning\\face_mask_model\\class_indices.pickle\n",
      "Class mapping: {0: 'with_mask', 1: 'without_mask'}\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Train the Model and Save Class Indices\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = validation_generator.samples // BATCH_SIZE\n",
    "\n",
    "print(\"[INFO] Training model...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "class_indices = train_generator.class_indices\n",
    "class_indices_flipped = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "with open(os.path.join(model_save_path, \"class_indices.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(class_indices_flipped, f)\n",
    "    print(f\"Class indices saved to {os.path.join(model_save_path, 'class_indices.pickle')}\")\n",
    "    print(f\"Class mapping: {class_indices_flipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3762b6eb-9f1e-465b-9b33-ad11a378dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:\\Machine Learning\\face_mask_model\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:\\Machine Learning\\face_mask_model\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to E:\\Machine Learning\\face_mask_model\\saved_model\n",
      "Model config saved to E:\\Machine Learning\\face_mask_model\\model_config.json\n",
      "Model summary saved to E:\\Machine Learning\\face_mask_model\\model_summary.txt\n",
      "[INFO] Model training complete!\n",
      "Final Training Accuracy: 0.9305\n",
      "Final Validation Accuracy: 0.9449\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Save Model and Print Accuracy\n",
    "\n",
    "# Save the model\n",
    "\n",
    "saved_model_path = os.path.join(model_save_path, \"saved_model\")\n",
    "tf.saved_model.save(model, saved_model_path)\n",
    "print(f\"Model saved to {saved_model_path}\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_save_path, \"model_config.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    print(f\"Model config saved to {os.path.join(model_save_path, 'model_config.json')}\")\n",
    "\n",
    "with open(os.path.join(model_save_path, \"model_summary.txt\"), \"w\") as summary_file:\n",
    "    model.summary(print_fn=lambda x: summary_file.write(x + '\\n'))\n",
    "    print(f\"Model summary saved to {os.path.join(model_save_path, 'model_summary.txt')}\")\n",
    "\n",
    "# Print final training and validation accuracy\n",
    "print(\"[INFO] Model training complete!\")\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8da5bf4-adda-44fe-bb1e-8fab4813537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Haar Cascade face detector: E:\\Machine Learning\\dataset\\haarcascade_frontalface_default.xml\n",
      "[INFO] Loading face mask detector model...\n",
      "Model loaded successfully\n",
      "Model signature: ['dense_3']\n",
      "Class indices: {0: 'with_mask', 1: 'without_mask'}\n"
     ]
    }
   ],
   "source": [
    "# Part 6: Load Face Detector and Mask Detector Model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "face_cascade_path = r\"E:\\Machine Learning\\dataset\\haarcascade_frontalface_default.xml\"\n",
    "if os.path.exists(face_cascade_path):\n",
    "    face_detector = cv2.CascadeClassifier(face_cascade_path)\n",
    "    print(f\"Using Haar Cascade face detector: {face_cascade_path}\")\n",
    "else:\n",
    "    print(\"[ERROR] Haar Cascade face detector not found\")\n",
    "    exit()\n",
    "\n",
    "model_save_path = r\"E:\\Machine Learning\\face_mask_model\"\n",
    "saved_model_path = os.path.join(model_save_path, \"saved_model\")\n",
    "\n",
    "print(\"[INFO] Loading face mask detector model...\")\n",
    "try:\n",
    "    mask_detector = tf.saved_model.load(saved_model_path)\n",
    "    print(\"Model loaded successfully\")\n",
    "    infer = mask_detector.signatures[\"serving_default\"]\n",
    "    print(\"Model signature:\", list(infer.structured_outputs.keys()))\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to load model: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(model_save_path, \"class_indices.pickle\"), \"rb\") as f:\n",
    "        class_indices = pickle.load(f)\n",
    "        print(\"Class indices:\", class_indices)\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Couldn't load class indices: {e}\")\n",
    "    class_indices = {0: \"with_mask\", 1: \"without_mask\"}\n",
    "    print(\"Using default class indices:\", class_indices)\n",
    "\n",
    "IMG_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55138dfc-7087-476e-a23d-23b8903eb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7: Detect and Classify Faces Function\n",
    "\n",
    "def detect_and_classify_faces(frame):\n",
    "    output_frame = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        if face_roi.size == 0:\n",
    "            continue\n",
    "        face_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        face_roi = cv2.resize(face_roi, (IMG_SIZE, IMG_SIZE))\n",
    "        face_roi = face_roi / 255.0\n",
    "        face_roi = face_roi.astype(np.float32)\n",
    "        face_roi = np.expand_dims(face_roi, axis=0)\n",
    "        try:\n",
    "            input_tensor = tf.convert_to_tensor(face_roi) #Corrected line\n",
    "            prediction = infer(input_tensor)\n",
    "            output_key = list(prediction.keys())[0]\n",
    "            pred_values = prediction[output_key].numpy()[0]\n",
    "            mask_index = np.argmax(pred_values)\n",
    "            confidence = pred_values[mask_index] * 100\n",
    "            if class_indices[mask_index] == \"with_mask\":\n",
    "                label = \"Mask\"\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                label = \"No Mask\"\n",
    "                color = (0, 0, 255)\n",
    "            label = f\"{label}: {confidence:.2f}%\"\n",
    "            cv2.rectangle(output_frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(output_frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf64b7b-619e-46af-aef7-11e7b5f464b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start webcam and display results\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    output_frame = detect_and_classify_faces(frame)\n",
    "    cv2.imshow(\"Face Mask Detection\", output_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72a4b9-b6e0-4948-ab93-546032803771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28679d73-8f92-471d-93d4-fefdd621b490",
   "metadata": {},
   "source": [
    "# **Face Mask Detection Using CNN and OpenCV**\n",
    "\n",
    "## **Objective**\n",
    "The objective of this project is to develop an **AI-based Face Mask Detector** that can classify whether a person is wearing a mask or not in real-time using a webcam. The system utilizes **Convolutional Neural Networks (CNNs)** for image classification and **OpenCV's Haar Cascade** for face detection.\n",
    "\n",
    "## **Steps Involved**\n",
    "### **1. Data Collection and Preprocessing**\n",
    "- Collected a dataset of images with and without face masks.  \n",
    "- Applied **image augmentation** techniques such as rotation, zooming, and flipping to improve model generalization.  \n",
    "- Rescaled images to **(64x64 pixels)** and normalized pixel values to a range of [0,1].  \n",
    "\n",
    "### **2. Model Building**\n",
    "- Built a **CNN model** using TensorFlow/Keras with:  \n",
    "  - **3 convolutional layers** with ReLU activation.  \n",
    "  - **Max-pooling layers** to reduce spatial dimensions.  \n",
    "  - **Dropout layer** to prevent overfitting.  \n",
    "  - **Fully connected layers** with softmax activation for classification.  \n",
    "- Compiled the model with **Adam optimizer** and **categorical cross-entropy loss function**.  \n",
    "\n",
    "### **3. Model Training**\n",
    "- Trained the model on **5,960 images** using **10 epochs** with a batch size of **8**.  \n",
    "- Achieved a final **training accuracy of 93.05%** and a **validation accuracy of 94.49%**.  \n",
    "- Saved the trained model and class indices for future inference.  \n",
    "\n",
    "### **4. Real-Time Face Mask Detection**\n",
    "- Loaded the trained model and class mapping.  \n",
    "- Used **OpenCV's Haar Cascade** to detect faces in webcam video frames.  \n",
    "- Preprocessed detected faces and fed them into the model for classification.  \n",
    "- Displayed real-time predictions on the video stream with bounding boxes and confidence scores.  \n",
    "\n",
    "## **Conclusion**\n",
    "- The developed model successfully detects **masked and unmasked faces** with high accuracy in real-time.  \n",
    "- The approach can be extended to **edge devices** like **Raspberry Pi, Jetson Nano**, or **mobile applications** for practical deployment.  \n",
    "- Future improvements can include **multi-class classification** (e.g., incorrectly worn masks) and **more robust face detection techniques** using deep learning-based detectors like **MTCNN or YOLO**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2ca45-b839-428c-afd0-2fb55ab1e010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ducat)",
   "language": "python",
   "name": "ducat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
